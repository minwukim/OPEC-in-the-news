library(dplyr)
library(jsonlite)
library(rvest)
library(glue)
rm(list = ls())
setwd("/Users/minwukim/Desktop/KDI Research/OPEC/OPEC-in-the-news/analysis")
apiKey <- "6BZtZvYLg0r5BhyUM2L4ONsAS0x5JMLt"
query = "opec"
begin_date = "20060818"
end_date = "20231117"
baseUrl <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?q={query}&begin_date={begin_date}&end_date={end_date}&sort=oldest"
baseUrl <- glue(baseUrl)
baseUrl
articles <- list()
for(i in 0:199){
# The following line will create the necessary URL by pasting together a few
# elements: the base URL, the page argument (which changes), and the key.
req <- fromJSON(paste0(baseUrl, "&page=", i, '&api-key=', apiKey), flatten=TRUE)
# This line tells the loop to give us a message telling us which page it is on
message("Retrieving page ", i)
# We will save the queried articles in the empty 'articles' object we created
articles[[i+1]] <- req$response$docs
# Because we do not want to overwhelm the API, let's make one search every
# 30 seconds.
Sys.sleep(12)
# Close the loop and run it! Look for the retrieving status in the console.
}
# First, combine all page results into one dataframe (and drop empty entries)
scrape <- rbind_pages(articles[sapply(articles, length)>0])
# Let's check the initial dataframe
nrow(scrape) # the number of rows should = 10*3 because there are 10 results per page
# Let's see what variables we have
colnames(scrape)
# View a snippet of the data and the class of each variable
as_tibble(scrape) # the web_url, for example, is 'character' class
scrape_simple <- scrape[, c("abstract", "web_url", "snippet", "lead_paragraph",
"print_section", "print_page", "pub_date", "_id",
"word_count", "uri")]
directory <- "nyt_scrape5.csv"
write.csv(scrape_simple, directory, row.names=FALSE)
rm(list = ls())
setwd("/Users/minwukim/Desktop/KDI Research/OPEC/OPEC-in-the-news/analysis")
apiKey <- "6BZtZvYLg0r5BhyUM2L4ONsAS0x5JMLt"
query = "opec"
begin_date = "20231109"
end_date = "20231117"
baseUrl <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?q={query}&begin_date={begin_date}&end_date={end_date}&sort=oldest"
baseUrl <- glue(baseUrl)
baseUrl
articles <- list()
for(i in 0:10){
# The following line will create the necessary URL by pasting together a few
# elements: the base URL, the page argument (which changes), and the key.
req <- fromJSON(paste0(baseUrl, "&page=", i, '&api-key=', apiKey), flatten=TRUE)
# This line tells the loop to give us a message telling us which page it is on
message("Retrieving page ", i)
# We will save the queried articles in the empty 'articles' object we created
articles[[i+1]] <- req$response$docs
# Because we do not want to overwhelm the API, let's make one search every
# 30 seconds.
Sys.sleep(12)
# Close the loop and run it! Look for the retrieving status in the console.
}
# First, combine all page results into one dataframe (and drop empty entries)
scrape <- rbind_pages(articles[sapply(articles, length)>0])
# Let's check the initial dataframe
nrow(scrape) # the number of rows should = 10*3 because there are 10 results per page
# Let's see what variables we have
colnames(scrape)
# View a snippet of the data and the class of each variable
as_tibble(scrape) # the web_url, for example, is 'character' class
scrape_simple <- scrape[, c("abstract", "web_url", "snippet", "lead_paragraph",
"print_section", "print_page", "pub_date", "_id",
"word_count", "uri")]
directory <- "nyt_scrape6.csv"
write.csv(scrape_simple, directory, row.names=FALSE)
